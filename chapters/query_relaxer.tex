\section{Algoritmo Progettato}
In questa sezione saranno illustrati sequenzialmente i passi effettuati dal software. I passi sono  seguenti:
\begin{itemize}[noitemsep]
    \item Caricamento dataset
    \item Caricamento lista RFD
    \item Ordinamento delle RFD
    \item Query Estesa
    \item Query Rilassata
\end{itemize}
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth,height=\textheight,keepaspectratio]{images/Flowchart.jpg}
\caption{Flowchart}\label{fig:4}
\end{figure}
\paragraph{Caricamento Dataset}
Il primo passo da effettuare è quello di caricare il DataSet in formato csv. Il separatore che di norma può essere costituito da un carattere a scelta è stato impostato come carattere ";".\\ Il DataSet può contenere vari tipi di dati: float, int, stringhe e date.
La struttura dati utilizzata per contenere i dati è quella del pandas.Dataframe , contenuta nel package Pandas\footnote{Pandas è una libreria software scritta per il linguaggio di programmazione Python .È adibita alla manipolazione e l'analisi dei dati. In particolare, offre strutture e operazioni di dati per la manipolazione di tabelle numeriche e di serie temporali. }. \\Il Dataframe di Pandas è una struttura dati tabulare bidimensionale variabile in formato e con assi etichettati (righe e colonne). Le operazioni vengono effettuate sulle righe e sulle colonne. 
\paragraph{Carimento lista RFD}
Il secondo passo consiste nel caricare la lista di RFD. L'utente può passare il path del file contenente le RFD, altrimenti viene invocato un algoritmo di ricerca di RFD sul DataSet indicato.
Nel caso in cui venga passato il path, il formato del file deve essere di tipo csv, con carattere di separazione uguale a ";"\footnote{In realtà è stato implementato un sistema capace di riconoscere ed utilizzare un qualsiasi carattere di separazione}.
Dato che ogni RFD può avere un attributo diverso come parte RHS , si è scelto di utilizzare una particolare notazione:
\begin{table}[H]
    \centering
    \begin{tabular}{l l l l l }
     RHS & $attr_1$ & $attr_2$ & $attr_{\ldots}$ & $attr_n$ \\
    $attr_{k}$ & $s_{12}$ & $s_{13}$ & $s_{\ldots}$ & $s_1n$  \\
    $attr_{j}$ & $s_{22}$ & $s_{23}$  & $s_{\ldots}$ & $s_{2n}$\\
    $attr_{p}$ & $s_{\ldots1}$ & $s_{\ldots3}$  & $s_{\ldots}$ & $attr_{r}$\\
    $s_{m1}$ & $s_{m2}$ & $s_{m3}$ & $s_{\ldots}$ & $s_{mn}$\\
    \end{tabular}
    \caption{Pandas Dataframe contenente le soglie delle RFD}
    \label{tab:RFD_notation}
\end{table}

La prima colonna del Dataframe contiene non una soglia bensì il nome dell'attributo che funge da RHS in quella RFD.
\\
Se l'utente non fornisce il path del file, o se il file non viene trovato, il software richiama l'algoritmo di ricerca delle RFD. Il path del Dataset viene passato all'algoritmo di RFDD\footnote{Relaxed Functional Dependencies Discovery} , dopo aver elaborato restituisce una Dataframe contenente le RFD, le quali vengono salvate su file e successivamente caricate in memoria.
\paragraph{Ordinamento delle RFD}
Una volta caricata la lista di query è necessario effettuare una pulizia sui dati.
Vengono eliminate le RFD che possiedono valore "NaN" su attributi che sono presenti nella query, dopodiché vengono eliminate le RFD dove nel lato RHS è presente un attributo della query \footnote{Utilizzare una RFD di questo tipo significherebbe utilizzare una dipendenza funzionale rilassata banale, la quale non porta alcuna informazione aggiuntiva}
\begin{table}[H]
    \centering
    \begin{tabular}{l l l l l }
     RHS & $attr_1$ & $attr_2$ & $attr_{\ldots}$ & $attr_n$ \\
    $attr_{k}$ & $s_{12}$ & $s_{13}$ & $s_{\ldots}$ & $s_1n$  \\
    $attr_{j}$ & $s_{22}$ & $s_{23}$  & $s_{\ldots}$ & $s_{2n}$\\
    $attr_{p}$ & $s_{\ldots1}$ & $s_{\ldots3}$  & $s_{\ldots}$ & $attr_{r}$\\
    $s_{m1}$ & $s_{m2}$ & $s_{m3}$ & $s_{\ldots}$ & $s_{mn}$\\
    \end{tabular}
    \caption{Pandas Dataframe contenente le soglie delle RFD}
    \label{tab:RFD_notation}
\end{table}
Dopo aver effettuato questa pulizia le RFD vengono ordinate. 
\newline
C'è stata molta indecisione riguardo l'implementazione dell'ordinamento, in quanto non è stata trovata una dimostrazione formale che ci indicasse quale metodo fosse il più efficace.
L'obiettivo è stato quello di trovare un algoritmo che ordinasse le RFD dando precedenza a quelle che producono Result Set non troppo ampi.
\\
Dopo vari test abbiamo pervenuto che il metodo più efficacie fosse quello di ordinare le RFDs prima in ordine decrescente rispetto al numero di attributi con valore "NaN" e poi in ordine crescente di soglia rispetto agli attributi presenti nella query. 
L'ordinare secondo soglie più piccole indica che quelle RFD agiscono su range ridotti, indicativamente ciò significa che rispetto ad un range più ampio un range di dimensioni ridotte include meno dati .In realtà può capitare che in un range di un DataSet vi sia una forte aggregazione di dati , superiore a tutti gli altri dati presenti nel Dataset. Ad esempio preso un Dataset che possiede un attributo altezza possiamo avere i seguenti valori
\begin{table}[H]
    \centering
    \begin{tabular}{l }
    altezza \\
    170 \\
    171 \\
    172 \\
    171 \\
    169 \\
    170 \\
    173 \\
    170 \\
    169 \\
    158 \\
    165 \\
    152 \\
    \end{tabular}
    \caption{Esempio di valori su attributo altezza}
    \label{tab:height_list}
\end{table}
In questo caso se abbiamo due range $[169,172]$ e $[140,165]$, l'intuizione ci dice che il primo range essendo il più piccolo dovrebbe includere meno dati, invece guardando dalla lista si può notare che il primo range contiene 8 valori ed il secondo solo 3.
Ciò capita abbastanza raramente , infatti nei test effettuati questo principio di ordinamento ha restituito quasi sempre i risultati migliori.
Per aumentare l'efficacia dell'ordinamento abbiamo ritenuto necessario dare precedenza alle RFD che nel lato LHS possiedono un numero di attributi il più simile possibile agli attributi presenti nella query.
Nella tabella sottostante viene mostrata parte di una lista di RFD ordinata \footnote{L'ordinamento è stato effettuato in base ad una query eseguita dall'utente, in questo caso la query è SELECT * FROM dataset{\_}string WHERE height=169}:
\begin{table}[H]
    \centering
    \begin{tabular}{l l l l l l l l}
        & RHS & age & height & shoe{\_}size & weight \\
        \hline
    0 & shoe{\_}size & NaN & 0.0 & 1.0 & NaN \\
    1 & weight & NaN & 0.0 & 0.0 & 1.0 \\
    2 & shoe{\_}size & 3.0 & 0.0 & 0.0 & NaN \\
    3 & weight & 6.0 & 1.0 & NaN & 4.0 \\
    4 & weight & NaN & 1.0 & 1.0 & NaN \\
    5 & shoe{\_}size & 6.0 & 1.0 & 1.0 & NaN \\
    6 & shoe{\_}size & NaN & 1.0 & 1.0 & 4.0 \\
    7 & shoe{\_}size & NaN & 1.0 & 0.0 & 2.0 \\
    8 & age & 6.0 & 1.0 & 0.0 & NaN \\
    9 & age & 5.0 & 1.0 & 1.0 & NaN \\
    \ldots & \ldots & \ldots & \ldots & \ldots & \ldots \\
    \end{tabular}
    \caption{Parte di una lista di RFD ordinata}
    \label{tab:ord_rdf}
\end{table}

\paragraph{Query Estesa}
D'ora in poi tutta la procedura descritta da qui in avanti verrà iterata per un numero di RFD pari ad un valore definito dall'utente. \footnote{In caso in cui l'utente lasci il campo vuoto, l'iterazione viene effettuata per ogni RFD presente nella lista}
Viene selezionata la i-esima RFD con $i\in[1,n]$. 
Si vanno a considerare gli attributi della RFD che siano presenti nella query\footnote{In questo momento si considerano solo gli attributi in LHS}, si prendono le soglie e si effettua una nuova una query estesa $Q_2$
Ad esempio supponendo che la query iniziale sia: \\~\\ \centerline{SELECT * FROM dataset{\_}string WHERE height=169} \\~\\ e che la RFD selezionata sia: \\~\\ \centerline{$(height \leq 0.0)  \rightarrow(shoe_size \leq 0.0)$} \\~\\
Viene effettuato un controllo sul tipo di valori degli attributi della query,
si vanno a identificare tipi string, int e float, questa identificazione serve a specificare quale funzione di distanza bisogna applicare durante l'estensione della query.
Dato che $height$ è un attributo di tipo numerico, si vanno a prendere tutti i valori che differiscono di 0 dal valore iniziale inserito dall'utente. In questo caso la query estesa $Q_2$ resta uguale a $Q_1$: \\~\\ 
\centerline{SELECT * FROM dataset{\_}string WHERE height =169} \\~\\
Effettuando una nuova interrogazione con la query $Q_2$ otteniamo un Result Set che sarà di sicuro $\geq$ del Result Set restituito dall query iniziale.
In questo caso entrambe le query restituiscono questo risultato: \newline 
\begin{table}[H]
    \centering
    \begin{tabular}{l l l l l l l l}
    n.row   & height & weight & shoe{\_}size & age \\
    \hline
    5 & 169 & 73 & 38 & 49 \\
    \end{tabular}
    \caption{Result Set restituito sia dalla query $Q_1$ e sia dalla query $Q_2$ }
    \label{tab:sta_ext_result_set}
\end{table}
\paragraph{Query Rilassata}
In questa procedura viene effettuato il vero e proprio rilassamento, scambiando gli attributi della query con attributi definiti dalla parte RHS della RFD selezionata (vedi Appendice [perfezionamento con lhs]).
Inizialmente si effettua una raccolta dei valori nel dataset, si selezionano solo gli attributi che sono presenti in RHS. Viene mantenuta una struttura dati\footnote{In questo momento lo definiamo come un set} per ogni tupla restituita dal dataset (vedi Appendice [perfezionamento combinazioni]). 
Per ogni Set vengono rilassati i valori in base alle soglie contenute nella RFD, il risultato sarà una serie di SET che contengono dei range più ampi.
È quindi possibile effettuare $k$ query rilassate, ottenendo vari Result Set. La loro unione andrà a costituire il Result Set finale che sarà restituito in output.
Riprendendo l'esempio riportato nel paragrafo precedente, vengono raccolti i dati sui valori presenti in RHS, in questo caso vi è solo l'attributo shoe{\_}sie \footnote{In realtà in RHS vi è sempre un unico attributo, la query viene rilassata con più attributi in quanto si esegue un perfezionamento vedi  Appendice [perfezionamento combinazioni]} che viene rilassato di una soglia pari a 0. Dato che la query estesa $Q_2$ ha restituito una sola tupla, verrà rilassato un unico valore. La query rilassata $Q_3$  è : \newline 
\centerline{SELECT * FROM dataset{\_}string WHERE shoe{\_}size=38}
\newline
Per la proprietà delle RFD (vedi Appendice Proprietà RFD) il Result Set restituito sarà $\geq$ del Result Set restituito dalla query estesa $Q_2$.

In questo caso il Result Set è uguale a :
\begin{table}[H]
    \centering
    \begin{tabular}{l l l l l l l l}
    n.row  & height & weight & shoe{\_}size & age \\
    \hline
    0  & 175 & 75 & 39 & 41 \\
    1  & 169 & 73 & 38 & 49 \\
    2  & 170 & 65 & 39 & 30 \\
    \end{tabular}
    \caption{Result Set restituito sia dalla query $Q_3$ }
    \label{tab:relax_result_set}
\end{table}

Dato che questa procedura viene effettuata per un numero N di RFD, viene effettuato un confronto in modo tale da restituire il Result Set che che sia il meno ampio possibile \footnote{Vengono scartati i Result Set che hanno cardinalità uguale ad Result restituito dalla query iniziale}.

\section{Struttura del Progetto}
Il progetto è incluso in una cartella contenente anche il software di ricerca di RFD, tutto il codice sviluppato è contenuto nella sottocartella query{\_}rewriter
\begin{figure}[H]
    \centering
    \includegraphics{struct_project.png}
    \caption{Struttura del Progetto}
    \label{fig:struct_project}
\end{figure}

\subsection{Package Dataset}
Il package Dataset include una serie di Dataset in formato $csv$ ed una cartella contente le rispettive liste di RFD già precalcolate. 
I Dataset inclusi nel progetto sono:
\begin{itemize}[noitemsep]
\let\labelitemi\labelitemii
    \item cars.csv
    \item cars2.csv
    \item cars{\_}db.csv
    \item cora.csv
    \item crawled-tweets.csv
    \item dataset.csv
    \item dataset{\_}string.csv
    \item dataset{\_}string2.csv
    \item echocardiogram.csv
    \item iris.csv
    \item restaurant
\end{itemize}

Sono state già calcolate le seguenti liste di RFD:
\begin{itemize}[noitemsep]
\let\labelitemi\labelitemii
    \item cars{\_}db{\_}rfds.csv
    \item cars{\_}rfds.csv
    \item cora{\_}rfds.csv
    \item dataset{\_}rdfs.csv
    \item dataset{\_}strng{\_}rdfs.csv
\end{itemize}


La maggior parte dei Dataset sono in formato numerico, ma ne sono presenti alcuni che possiedono attributi sia in formato numerico che di tipo stringa

\subsection{Package io}
In questo package sono contenuti diversi moduli divisi in baso al loro scopo. Vi sono due sottocartelle che raggruppano il tutto.
Il contenuto nella sottocartela csv comprende:
\begin{itemize}[noitemsep]
\let\labelitemi\labelitemii
    \item csv{\_}parser.py
    \item io.py
\end{itemize}

\paragraph{csv{\_}parser.py}
Questo modulo contiene la classe CSV Parser che si occupa di prendere i dati dai file CSV e salvarli in un dataframe, il tutto viene fatto nel metodo init.
\begin{listing}[H]
\begin{minted}[frame=lines,linenos]{python}
class CSVParser:
    def __init__(self, csv_path: str):
        self.path = csv_path
        self.delimiter = self.__guess_delimiter()
        self.data_frame = pd.read_csv(self.path, 
                                delimiter=self.delimiter)
        self.rows_count, self.columns_count = self.data_frame.shape
        self.header = list(self.data_frame)
\end{minted}
\caption{Class CSVParser}
\label{Code:1}
\end{listing}
L'operazione vera e propria di $parsing$ e $loading$ dei dati viene delegata al Framework Pandas, richiamando il metodo alla riga 5
Le variabili di questa classe sono:
\begin{itemize}[noitemsep]
\let\labelitemi\labelitemii
    \item path:[str] Contiene il path del file CSV
    \item delimiter:Contiene il carattere di separazione
    \item data{\_}frame:[Pandas Dataframe] Contiene i dati del file CSV letto
    \item rows{\_}count:[int]contiene il numero di righe contenute nel file CSV
    \item column{\_}count:[int]contiene il numero di colonne contenute nel file CSV
    \item header:[list] contiene una lista di valori che fungono da header, questi valori sono specificati nella prima riga del file csv
\end{itemize}
L'unico argomento da passare al costruttore è il path del file, tutti gli altri dati vengono calcolati. 
All'interno del costruttore è presente una chiamata al metodo {\_\_}guess{\_}delimiter:

\begin{listing}[H]
\begin{minted}[frame=lines,linenos]{python}
    def __guess_delimiter(self):
\end{minted}
\caption{Guess delimiter}
\label{Code:2}
\end{listing}

Questo metodo ha il compito di indovinare il carattere di separazione leggendo il file.

\paragraph{io.py}
Questo modulo contiene la classe CSVInputOutput la quale si occupa di importare/esportare una Pandas.Dataframe da/in un file CSV
\begin{listing}[H]
\begin{minted}[,frame=lines,linenos]{python}
    class CSVInputOutput:
\end{minted}
\caption{CSVInputOutput}
\label{Code:3}
\end{listing}
Le variabili di questa classe sono:
\begin{itemize}[noitemsep]
    \item sep:[str] campo contenente il simbolo di separazione utilizzato nei file CSV
    \item na{\_}rep:[str] campo contenente il simbolo che rappresenta un dato mancante 
    \item index:[boolean] flag che indica se si vuole stampare nel file csv gli indici di riga
    \item encoding:[str] campo contente la codifica dei file input/output
    \item date{\_}format:[str] campo contente il formato delle date
\end{itemize}
Questa classe possiede due metodi.
Il primo è il metodo store
\begin{listing}[H]
\begin{minted}[bgcolor=black,frame=lines,linenos]{python}
    def store(self, df: pd.DataFrame, path: str):
\end{minted}
\caption{store method}
\label{Code:4}
\end{listing}
Questo metodo prende in input il path del del file in cui andare a salvare il DataFrame

Il secondo è il metodo load
\begin{listing}[H]
\begin{minted}[bgcolor=black,frame=lines,linenos]{python}
    def load(self, path: str) -> pd.DataFrame:
\end{minted}
\caption{load method}
\label{Code:5}
\end{listing}
Il metodo prende in input il path del file e delegando a Pandas Dataframe esegue il parsing e il loading


La seconda sottocartella rfd contiene vari moduli inerenti alla manipolazione delle RFD:
\begin{itemize}[noitemsep]
\let\labelitemi\labelitemii
    \item rfd{\_}extractor.py
    \item store{\_}and{\_}load.py
\end{itemize}

\paragraph{rfd{\_}extractor.py}
È contiene la classe RFDExtractor che si occupa di estrarre le RFD da un dataset
rfd{\_}extractor.py
\begin{listing}[H]
\begin{minted}[bgcolor=black,frame=lines,linenos]{python}
class RFDExtractor:
    ...
    def __init__(self, args, debug_mode=False) -> None:
    ...
    def __str__(self) -> str:
    ...
    def extract_args(self, args):
    ...
    def extract_hss(self, cols_count, lhs, rhs):
    ...
    def extract_sep_n_header(self, c_sep, csv_file, has_header):
    ...
    def check_correctness(self, has_dt, hss, index_col):
    ...
    def usage(self):
    ...
    def print_human(self, rfd_data_frame: pd.DataFrame):
    ...
    def get_rfd_dictionary_list(self) -> list:
    ...
\end{minted}
\caption{RFDExtractor}
\label{Code:6}
\end{listing}

Non saranno fornite particolari informazioni su questa classe , in quanto essa fa parte del progetto riguardante la ricerca di RFD , per ulteriori informazioni si veda \cite{tesinaIA}

\paragraph{store{\_}and{\_}load.py}
È un modulo che ha funzione di wrapping per l'algoritmo di ricerca di RFD.
Contiene due metodi:
\begin{listing}[H]
\begin{minted}[bgcolor=black,frame=lines,linenos]{python}
def diff(list1: list, list2: list):
...
def search_rfds(csvPath,name_rfds_file):
\end{minted}
\caption{Metodi store{\_}load{\_}rfds}
\label{Code:7}
\end{listing}

IL primo metodo "diff" prende in input due liste, e restituisce una lista di elementi che sono presenti nella prima ma non nella seconda.

Il secondo metodo richiama l'algoritmo di Ricerca delle RFD \footnote{In questo caso non è semplice richiamo di una funzione, bensì vengono richiamati una serie di metodi, e mano a mano vengono effettuate delle elaborazioni sui dati} e crea un file contente una lista di RFD. Prende in input il path del file contenente il Dataset, e una stringa che contiene il nome del file che incapsula le RFD. 

\subsection{Package query}
Nel package query vi sono tre moduli, tutte e tre si occupano di svolgere operazioni riguardanti le interrogazioni al dataset.
I tre moduli sono:
\begin{itemize}[noitemsep]
\let\labelitemi\labelitemii
    \item relaxer.py
    \item slicer.py
\end{itemize}

\paragraph{relaxer.py}
Questo modulo contiene la classe QueryRelaxer, è una classe ausiliaria, contiene dei metodi statici che si occupano di rilassare la query rispetto ad una Relaxed Functional Dependency. La classe contiene i seguenti metodi:

\begin{listing}[H]
\begin{minted}[frame=lines,linenos]{python}
class QueryRelaxer:
    @staticmethod
    def drop_query_nan(rfds_df: pd.DataFrame, query: dict) 
                                            -> pd.DataFrame:
    ...
    @staticmethod
    def drop_query_rhs(rfds_df: pd.DataFrame, query: dict)
                                            -> pd.DataFrame:
    ...
    @staticmethod
    def sort_by_decresing_nan_incresing_threshold(rfds_df: 
                pd.DataFrame, query: dict) -> pd.DataFrame:
    ...
    @staticmethod
    def sort_by_increasing_threshold(rfds_df: pd.DataFrame,
        data_set: pd.DataFrame, query: dict) -> pd.DataFrame:
    ...
    @staticmethod
    def rfd_to_string(rfd: dict) -> str:
    ...
    @staticmethod
    def query_dict_to_expr(query: dict) -> str:
    ...
    @staticmethod
    def extend_query_ranges(query: dict, rfd: dict, 
            data_set: pd.DataFrame = None) -> dict:
    ...
    @staticmethod
    def similar_strings(source: str, data: pd.DataFrame,
                        col: str, threshold: int) -> list:
    ...
    @staticmethod
    def extract_value_lists(df: pd.DataFrame, columns: list):
    ...
\end{minted}
\caption{Classe Query Relaxer}
\label{Code:10}
\end{listing}

Il metodo drop{\_}query{\_}na(...) si occupa di eliminare tutte le RFD che possiedono valori NaN su attributi che sono contenuti anche nella query. Il metodo prende in input un pandas.Dataframe contenente la lista di RFD ed un dizionario contenente la query, restituisce un altro pandas.Dataframe.

Il metodo drop{\_}query{\_}rhs(...) si occupa di eliminare tutte le RFD che possiedono un attributo della query come parte RHS. Il metodo prende in input un pandas.Dataframe contenente la lista di RFD ed un dizionario contenente la query, restituisce un altro pandas.Dataframe.

Il metodo sort{\_}by{\_}decreasing{\_}nan{\_}incresing{\_}threshold(...), contiene un algoritmo di ordinamento. Il metodo prende in input un pandas.Dataframe ordina prima per numero di NaN presenti e poi per ordine crescente rispetto alla soglie, restituisce un pandas.Dataframe.

Il metodo sort{\_}by{\_}increasing{\_}threshold(...) , contiene un altro algoritmo di ordinamento, che è stato utilizzato solo nei test, dove è risultato poco producente. Il metodo prende in input un pandas.Dataframe e un dizionario contenente la query. Restitusce un altro pandas.Dataframe ordinato solo rispetto alle soglie degli attributi \footnote{In entrambi gli algoritmi di ordinamento vengono ordinate prima le soglie degli attributi contenuti nella query e poi in caso di valori uguali si ordina rispetto alle soglie degli altri attributi in LHS}

il metodo rfd{\_}to{\_}string(...) svolge un ruolo alquanto semplice, si occupa di convertire il formato delle RFD in un formato stringa facilmente leggibile.

Questo metodo converte il dizionario nel formato stringa richiesto dal Dataframe di Pandas.
Qui è importante fare delle precisazioni dato che in  questa parte del codice vengono definite le condizioni che si possono applicare alla query.
Nella riga 9 viene implementato la funzionalità di selezione di valori appartenenti ad un range. Possiamo effettuare una query del tipo: \\~\\
\centerline{SELECT * FROM dataset{\_}string WHERE height BETWEEN $value_1$ and $value_2$}
\\~\\
Nella riga 12 viene implementata la funzionalità di uguaglianza per int e float.
Nelle righe dal 13 a 22, viene implementata la funzione di uguaglianza con le stringhe, inoltre è stata implementata anche la funzionalità simil-SQL "LIKE". L'utente può quindi effettuare delle query non solo di uguaglianza, ma anche di contenimento. Ad esempio può chiedere tutte le stringhe che iniziamo per "mary" o che contengono la parola "ohn"

\begin{listing}[H]
\begin{minted}[frame=lines,linenos]{python}
def to_expression(self) -> str:
    last_key = list(self.keys())[-1]
    expr = ""
    for k, v in self.items():
        if isinstance(v, range):
            ...
        elif isinstance(v, dict):
            expr += " {} >= {} and {} <= {}".format(k, v['min'],
                                                    k, v['max'])
        elif isinstance(v, (int, float, list)):
            expr += " {} == {}".format(k, v)
        elif isinstance(v, str):
            if "%" in v:
                if v.startswith("%") and v.endswith("%"):
                    expr += k + ".str.contains('{}') ".format(v[1:-1])
                elif v.startswith("%"):
                    expr += k + ".str.endswith('{}') ".format(v[1::])
                elif v.endswith("%"):
                    expr += k + ".str.startswith('{}') ".format(v[:-1])
            else:
                expr += " {} == {}".format(k, v)

        if k is not last_key:
            expr += " and "
    return expr
\end{minted}
\caption{Metodo def{\_}to{\_}express()}
\label{Code:9}
\end{listing}

Il metodo extend{\_}query{\_}ranges(...) prende in input una query ed una RFD ed estende i range sugli attributi basandosi sulle soglie contenute nella RDF. Nel caso in qualche attributo della query è di tipo stringa vengono calcolate le stringhe simili e vengono utilizzare nella query estesa.
(vedi lavori futuri)

Il metodo similar{\_}string() prende in input una stringa, una pandas.Dataframe ed una soglia e si occupa di trovare nel database tutte le stringhe che differiscono di una soglia $\epsilon$. Restituisce un altro pandas.Dataframe 

Il metodo extraxt{\_}value{\_}list(...) prende in input un pandas.Dataframe ed una lista contenente le colonne di cui si vogliono estrarre i valori. Questo metodo restituisce un dizionario di liste, ognuna delle quali ha come chiave il nome dell'attributo

\paragraph{slicer.py}
È una classe ausiliaria che contiene un solo metodo.
\begin{listing}[H]
\begin{minted}[frame=lines,linenos]{python}
class Slicer:
    @staticmethod
    def slice(df: pd.DataFrame) -> list:
\end{minted}
\caption{Metodo def{\_}to{\_}express()}
\label{Code:14}
\end{listing}

Il metodo slice() si occupa di dividere un pandas.Dataframe, ogni tupla andrà a formare un nuovo Dataframe, viene restituita una lista di DataFrame.

\subsection{Modulo main.py}
Modulo che lancia il programma vero e proprio. Richiede
l'inserimento del nome di un Dataset, assieme ad una serie di opzioni
facoltative, secondo la seguente struttura:
\begin{listing}[H]
\begin{minted}[frame=lines,linenos]{python}
python main .py -p <path-of-dataset > -q <query> [ options ]
\end{minted}
\end{listing}

dove:
\begin{itemize}
\item path-of-dataset è il path relativo Dataset da utilizzare per le query;
\item query è il parametro contenente la query, essa può avere la seguente struttura\footnote{I valori nel caso siano in formato stringa vanno racchiusi fra apici singoli. Ciò viene fatto per evitare problemi nella lettura di eventuali spazi bianchi.}:
\begin{itemize}
\item query semplice : \mint{json}|"{'nome_attributo',valore_attributo}"|
\item query con funzione like : \mint{json}|"{'nome_attributo','%value%'}"|
\item query con controllo su range :  \mint{json}|"{'nome_attributo',{'min':min_val,'max':max_val}}"|
\end{itemize}
\end{itemize}
e [options] può contenere zero o più delle seguenti opzioni:

\begin{itemize}
\item -r $<$rfds-path$>$ : nome del file in cui sono salvate le RFD, se non viene fornito le RFD vengono calcolate
\item -n $<$number-of-test$>$ : numero di RFD da testare, se non viene fornito il test viene effettuato per tutte le RFD contenute nella lista
\item -o $<$path-output$>$ : nome del file in cui salvare l'output del processo di Query Relaxing
\end{itemize}


\paragraph{Metodo main()}
Il metodo main è il primo metodo ad essere eseguito di occupa di leggere e controllare alcuni parametri passati da linea di comando. Una volta processati esso richiama il metodo start{\_}process()

\paragraph{Metodo start{\_}process()}
Questo metodo rapprensenta il core del software, incapsula tutti i processi di Query Relaxation.
Segue ora una lista dei metodi che vengono richiamati:
\begin{enumerate}
    \item Inizialmente si occupa di leggere il DataSet richiamando i metodi della classe CSVInputOutput.
    \item Viene effettuato il carimento della lista di RFD, se tale file non esiste viene generato
    \item Viene effettuato un cleaning del DataFrame contenente le RFD, dopodichè viene ordinato
    \item Per ogni RFD i contenuta nella lista \footnote{Vi è un parametro per specificare il numero massimo di RFD da testare} si itera nel seguente modo :
    \begin{itemize}
        \item Si effettua una query estesa rispetto alle soglie della RFD i-esima
        \item Vengono caricati ,dal Result Set ,tutti i valori degli attributi che sono nella parte RHS della query, inoltre vengono caricati anche i valori degli attributi LHS che non sono nella query. Viene mantenuto un DataFrame per ogni tupla risultante nel Result Set
        \item Tutti i valori nei DataFrame generati precedentemente vengono estesi rispetto alla soglie della RFD i-esima.
        \item Viene effettuata una query unendo (con degli or) tutte le condizioni contenute nel DataFrame
        \item Il Result Set contenuto dalla query rilassata viene salvato in memoria.
    \end{itemize}
    \item Viene restituito in output il Result Set meno ampio possibile, che sia però di cardinalità maggiore a quello restituito dalla query originale. Il Result set viene automaticamente salvato in un file formato JSON
\end{enumerate}


\section{Installazione}
\paragraph{Requisiti}
La versione di Python utilizzata è la 3.6.2 , è stata utilizzati su di un architettura a x64.
Dato che buona parte del codice richiama metodi scritti in Cython , serve un compilatore C++. È stato utilizzato un compilatore della Microsoft incluso nel pacchetto Visual Studio Community 2017 \\
Il progetto contiene le seguenti dipendenze:
\begin{itemize}[noitemsep]
\item[-] click $\geq$ 6.7
\item[-] Cython $\geq$ 0.25.2
\item[-] Flask $\geq$ 0.12.2
\item[-] itsdangerous $\geq$ 0.24
\item[-] Jinja2 $\geq$ 2.9.6
\item[-] MarkupSafe $\geq$ 0.23
\item[-] matplotlib $\geq$ 2.0.2
\item[-] nltk $\geq$ 3.2.4
\item[-] numpy $\geq$ 1.13.0
\item[-] pandas $\geq$ 0.19.2
\item[-] pyparsing $\geq$ 2.2.0
\item[-] python-dateutil $\geq$ 2.6.0
\item[-] pytz $\geq$ 2017.2
\item[-] six $\geq$ 1.10.0
\item[-] tornado $\geq$ 4.5.1
\item[-] Werkzeug $\geq$ 0.12.2
\item[-] editdistance $\geq$ 0.3.1
\item[-] cycler $\geq$ 0.10.1
\item[-] pip $\geq$ 9.0.1
\item[-] setuptools $\geq$ 36.5.0
\item[-] rfd-discovery $\geq$ 0.0.1
\end{itemize}
\paragraph{Installazione}

\begin{enumerate}
\item Scaricare Python (link: https://www.python.org/downloads/release/python-362/), fare attenzione a scegliere la versione in base all'architettura della macchina su cui verrà eseguito il software.
\item Scaricare ed installare il pacchetto "Sviluppo applicazioni desktop con C++" ottenibile da Visual Studio Community 2017 (link: https://www.visualstudio.com/it/thank-you-downloading-visual-studio/?sku=Community\&rel=15)
\item Aggiugere alla variabile di ambiente PATH i seguenti valori:\\ "D:\textbackslash Programmi\textbackslash VS\textbackslash VC\textbackslash Tools\textbackslash MSVC\textbackslash 14.11.25503\textbackslash bin\textbackslash HostX64\textbackslash x64"\\
"C:\textbackslash Program Files (x86)\textbackslash Windows Kits\textbackslash 10\textbackslash bin\textbackslash x64"\footnote{La posizione di tali cartelle può variare}
\item Aprire il "Prompt dei comandi degli strumenti nativi x64(o x86 per sistemi a 32 bit) per VS 2017" ed eseguire il comando "python -m pip install -U pip setuptools"
\item Scaricare il progetto (link: https://github.com/izio7/AttributedGraphProfiler)
\item Spostarsi all'interno della Root del progetto ed aprire il terminale.
\item Eseguire il comando "virtualenv venv" per creare un Virtual Environment in cui eseguire il progetto
\item Eseguire il comando "venv\textbackslash Scripts \textbackslash activate" con cui si si accede al Virtual Environment \footnote{Sotto ambiene Linux\textbackslash MacOS il comando da eseguire è "source venv/bin/activate"}
\item Eseguire il comando "pip install -r requirements.txt" per installare le dipendenze richieste.\footnote{Se si esegue il software in un ambiente di sviluppo dedicato questa operazione viene effettuata automaticamente}
\item Eseguire il comando "python setup.py install"
\item Eseguire i comando "python build.py build{\_}ext --inplace" per effettuare generare il codice C tramite Cython
\item Ora il software è pronto per eseguire le Query rilassate, il main si trova all'interno della folder "query{\_}rewriter"
\end{enumerate}


\section{Test}
In questa sezione sono raccolti tutti i test effettuati sul progetto.

